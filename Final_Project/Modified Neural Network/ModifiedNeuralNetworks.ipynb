{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Neural Network on Pedestrian Dynamics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is a part of MLCMS Winter 2023-24 Final Project. Designed and Developed by Gaurav Vaibhav and team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing required packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are importing the required packages and the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PedNeuralNetwork import PedNeuralNetwork\n",
    "from torch.utils.data import  TensorDataset\n",
    "\n",
    "from generated_data.K_10 import *\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Testing Training Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are defining a function which takes input arguments as datasets for training and testing along with train/test sizes. It returns training and testing dataset inputs(2*10+1) and targets (observed velocity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def train_test_dataset(*args):\n",
    "\n",
    "    dataset1, dataset2, train_size, test_size = args\n",
    "    train_dataset = [dataset1[i] for i in torch.randperm(train_size)]\n",
    "    test_dataset = [dataset2[i] for i in torch.randperm(test_size)]\n",
    "    \n",
    "    train_dataset = torch.stack([sample for sample in train_dataset])\n",
    "    test_dataset = torch.stack([sample for sample in test_dataset])\n",
    "    X_train = train_dataset[:,:-1]\n",
    "    Y_train = train_dataset[:,-1]\n",
    "    X_test = test_dataset[:,:-1]\n",
    "    Y_test= test_dataset[:,-1]\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining the model and its hyperparams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are declaring hyperparameters for the model and assigning the values (after trial-and-error iteration) following by instantiating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\"batch_size\" : 10,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"decay\" : 0.0001,\n",
    "           \"epochs\" : 10}\n",
    "\n",
    "input_size = 21\n",
    "output_size = 1\n",
    "hidden_sizes = [3]      #List of the dimensions of hidden layers\n",
    "\n",
    "# Instantiate the model\n",
    "model = PedNeuralNetwork(input_size, hidden_sizes, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are training the model with three necessary steps: Forward, Backward, Updating the model weights. Scheduler is used to display the progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "def train_model(model, train_loader, loss_func, epochs=10):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=hparams[\"learning_rate\"],\n",
    "                                          weight_decay=hparams[\"decay\"])\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs * len(train_loader) / 5, gamma=0.7)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Train\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch}/{epochs}]')\n",
    "        training_loss = 0\n",
    "            \n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad()\n",
    "            data = batch[0]\n",
    "            vel = batch[1]\n",
    "            predicted_vel = model(data) #Forward().\n",
    "            loss = loss_func(predicted_vel, vel) # Compute the loss over the predictions and the ground truth.\n",
    "            loss.backward()  # Backward().\n",
    "            optimizer.step() # Update the parameters.\n",
    "            \n",
    "            training_loss += loss.item()\n",
    "            scheduler.step()\n",
    "            training_loop.set_postfix(train_loss = \"{:.8f}\".format(training_loss / (train_iteration + 1)))\n",
    "            # Update the progress bar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, We have defined function for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss = 0\n",
    "    pred_tensor = []\n",
    "    target_tensor = []\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]\n",
    "        targets = batch[1]\n",
    "        preds = model.forward(inputs)\n",
    "        loss += criterion(\n",
    "            torch.squeeze(targets), torch.squeeze(preds)\n",
    "        ).item()\n",
    "        pred_tensor.append(preds)\n",
    "        target_tensor.append(targets)\n",
    "    \n",
    "    pred_tensor = torch.cat(pred_tensor, dim=0)\n",
    "    target_tensor = torch.cat(target_tensor, dim=0)\n",
    "\n",
    "    return pred_tensor, loss/(len(dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Losses for different training/testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are determining the testing errors for 7 training/testing dataset combinations: R/R, B/B, R/B, B/R, R+B/R, R+B/B, R+B/R+B and plotting the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model for  R/R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [0/10]:  53%|███████████████████████████████▌                            | 10683/20302 [00:52<00:46, 204.97it/s, train_loss=0.04994799]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5848\\3125037117.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5848\\3729321497.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, loss_func, epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mvel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mpredicted_vel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Forward().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_vel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Compute the loss over the predictions and the ground truth.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Backward().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Update the parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gaura\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gaura\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gaura\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3295\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "R_dataset = np.array(pd.read_csv(\"generated_data/K_10/R_230.csv\", sep=',',header=1))        #Provide the suitable file for Ring scenario\n",
    "B_dataset = np.array(pd.read_csv(\"generated_data/K_10/B_095.csv\", sep=',',header=1))        #Provide the suitable file for Bottleneck scenario\n",
    "R_dataset[:,:21] = R_dataset[:,:21]/100\n",
    "B_dataset[:,:21] = B_dataset[:,:21]/100\n",
    "R_dataset = torch.tensor(R_dataset, dtype=torch.float32)\n",
    "B_dataset = torch.tensor(B_dataset, dtype=torch.float32)\n",
    "RB_dataset = torch.concat([R_dataset, B_dataset])\n",
    "RB_dataset = [RB_dataset[i] for i in torch.randperm(len(RB_dataset))]\n",
    "RB_dataset = RB_dataset[:int(0.5*len(RB_dataset))]\n",
    "\n",
    "train_list = [R_dataset, B_dataset, R_dataset, B_dataset, RB_dataset, RB_dataset, RB_dataset]\n",
    "test_list =  [R_dataset, B_dataset, B_dataset, R_dataset, R_dataset, B_dataset, RB_dataset]\n",
    "test_error = []\n",
    "test_labels = ['R/R', 'B/B', 'R/B', 'B/R', 'R+B/R', 'R+B/B', 'R+B/R+B']\n",
    "\n",
    "for i in range(len(train_list)):\n",
    "    print('Training the model for ', test_labels[i])\n",
    "    train_size = int(len(train_list[i]))\n",
    "    test_size = int(len(test_list[i])*0.5)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_dataset(train_list[i], test_list[i], train_size, test_size)\n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    Y_test = Y_test.reshape(-1,1)\n",
    "    #print('Length of total dataset',len(train_list[i]))\n",
    "    #print('Length of training dataset', X_train.shape)\n",
    "    #print('Length of testing dataset',  X_test.shape)\n",
    "\n",
    "    #Dropping the few last datasets after creating batches\n",
    "    batch_size=hparams['batch_size']\n",
    "    nsamples = len(X_train) // batch_size\n",
    "    trim_length = nsamples * batch_size\n",
    "    train_dataset = TensorDataset(X_train[:trim_length], Y_train[:trim_length])\n",
    "\n",
    "    # Loading the training dataset into batches\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    epochs = hparams.get('epochs', 20)\n",
    "    loss_func = nn.MSELoss()\n",
    "    train_model(model, train_loader, loss_func, epochs=epochs)\n",
    "    \n",
    "    test_dataset = TensorDataset(X_test, Y_test)\n",
    "    pred_tensor, test_score = evaluate_model(model, test_dataset)\n",
    "    test_error.append(test_score)\n",
    "    print(\"Testing Error for \", test_labels[i], \" : \", test_error[i],\"\\n\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_labels, test_error)\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Testing errors for diff. datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ring and Bottleneck predicted speed trend for R+B/R+B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As given int the paper, we are evaluating the trend of predicted speeds for Ring and Bottleneck scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CustomDataset1, CustomDataset2\n",
    "\n",
    "def mixed_dataset():\n",
    "\n",
    "    R_dataset = np.array(pd.read_csv(\"generated_data/K_10/R_230.csv\", sep=',',header=1))\n",
    "    B_dataset = np.array(pd.read_csv(\"generated_data/K_10/B_095.csv\", sep=',',header=1))\n",
    "    R_dataset[:,:21] = R_dataset[:,:21]/100\n",
    "    B_dataset[:,:21] = B_dataset[:,:21]/100\n",
    "    R_dataset = torch.tensor(R_dataset, dtype=torch.float32)\n",
    "    B_dataset = torch.tensor(B_dataset, dtype=torch.float32)\n",
    "\n",
    "    # Combing both datasets with identifier (to identify particular dataset from NN output belongs to R or B)\n",
    "    dataset1 = CustomDataset1(R_dataset)\n",
    "    dataset2 = CustomDataset2(B_dataset)\n",
    "\n",
    "    # Combine the datasets\n",
    "    RB_dataset = dataset1 + dataset2\n",
    "    RB_dataset = [RB_dataset[i] for i in torch.randperm(len(RB_dataset))]\n",
    "\n",
    "    #Splitting 50% training and test data\n",
    "    data = [data for data,_ in RB_dataset]\n",
    "    data = torch.stack(data)\n",
    "    split_length = int(len(data)*0.5)\n",
    "    train_dataset = data[:split_length,:]\n",
    "    test_dataset = data[split_length+1:,:]\n",
    "\n",
    "    #Storing the identifiers R/B for combined datasets\n",
    "    identifiers = [identifier for _,identifier in RB_dataset]\n",
    "    identifiers = torch.tensor(identifiers)\n",
    "    test_identifier = identifiers[split_length+1:]\n",
    "    R_indices = [index for index, elem in enumerate(test_identifier) if elem==1]\n",
    "    B_indices = [index for index, elem in enumerate(test_identifier) if elem==2]\n",
    "\n",
    "    X_train = train_dataset[:,:-1]\n",
    "    Y_train = train_dataset[:,-1]\n",
    "    X_test = test_dataset[:,:-1]\n",
    "    Y_test= test_dataset[:,-1]\n",
    "\n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    Y_test = Y_test.reshape(-1,1)\n",
    "    #print('Length of total dataset', train_size+test_size)\n",
    "    #print('Length of training dataset', X_train.shape)\n",
    "    #print('Length of testing dataset',  X_test.shape)\n",
    "\n",
    "    #Dropping the few last datasets after creating batches\n",
    "    batch_size=hparams['batch_size']\n",
    "    nsamples = len(X_train) // batch_size\n",
    "    trim_length = nsamples * batch_size\n",
    "    train_dataset = TensorDataset(X_train[:trim_length], Y_train[:trim_length])\n",
    "\n",
    "    # Loading the training dataset into batches\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    #Training and evaluating the model\n",
    "    loss_func = nn.MSELoss()\n",
    "    train_model(model, train_loader, loss_func, epochs=epochs)\n",
    "    test_data = TensorDataset(X_test, Y_test)\n",
    "    pred_vel, test_score = evaluate_model(model, test_data)\n",
    "\n",
    "    #Concatenating the pred. speed with identifier\n",
    "    R_predVel = torch.cat([test_dataset[R_indices,0].view(-1,1), pred_vel[R_indices,:]], dim=1)\n",
    "    B_predVel = torch.cat([test_dataset[B_indices,0].view(-1,1), pred_vel[B_indices,:]], dim=1)\n",
    "    R_predVel[:,0] /= R_predVel[:,0].mean()\n",
    "    B_predVel[:,0] /= B_predVel[:,0].mean()\n",
    "\n",
    "    return test_score, R_predVel, B_predVel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the predicted speed curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating results from R+B/R+B\n",
    "test_score, R_predVel, B_predVel = mixed_dataset()\n",
    "\n",
    "# Fitting a polynomial curve (adjust the degree as needed)\n",
    "degree = 2\n",
    "R_predVel = R_predVel.detach().numpy()\n",
    "B_predVel = B_predVel.detach().numpy()\n",
    "Rcoefficients = np.polyfit(R_predVel[:,0], R_predVel[:,1], degree)\n",
    "Bcoefficients = np.polyfit(B_predVel[:,0], B_predVel[:,1], degree)\n",
    "Rpoly = np.poly1d(Rcoefficients)\n",
    "Bpoly = np.poly1d(Bcoefficients)\n",
    "\n",
    "# Generating points for the fitted curve\n",
    "Rx_fit = np.linspace(0.5, 3.5, 100)\n",
    "Ry_fit = Rpoly(Rx_fit)\n",
    "Bx_fit = np.linspace(0.5, 3.5, 100)\n",
    "By_fit = Bpoly(Bx_fit)\n",
    "\n",
    "# Plotting the scattered points and the fitted curve\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.scatter(R_predVel[:,0], R_predVel[:,1])\n",
    "plt.scatter(B_predVel[:,0], B_predVel[:,1])\n",
    "plt.plot(Rx_fit, Ry_fit, 'b', label=f'Ring')\n",
    "plt.plot(Bx_fit, By_fit, 'r', label=f'Bottleneck')\n",
    "\n",
    "#axes titles\n",
    "plt.xlim(0.5,3.5)\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.legend()\n",
    "plt.xlabel('Mean Spacing, m')\n",
    "plt.ylabel('Speed, m/s')\n",
    "plt.title('Pred. Velocity comparison for Ring & Bottleneck')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Testing errors for different model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the paper, we are computing the testing errors against the model complexities. Though, it doesn't fully replicate the results of paper, it still convinces that the network of 1 hidden layer with 3 nodes is sufficient enough for this problem and two layers are not helping much to improve the efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [[1],[2],[3],[4,2],[6,3],[10,4]]      #List of the dimensions of hidden layers\n",
    "hidden_sizes_labels = ['[1]', '[2]', '[3]', '[4,2]', '[6,3]', '[10,4]']\n",
    "test_scores = []\n",
    "\n",
    "for i in range(len(hidden_sizes)):\n",
    "    model = PedNeuralNetwork(input_size, hidden_sizes[i], output_size)\n",
    "    test_score, R_predVel, B_predVel = mixed_dataset()\n",
    "    print('Testing error for ', hidden_sizes_labels[i],' : ', test_score,'\\n')\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(hidden_sizes_labels, test_scores)\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Testing errors for diff. model network')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "54970da6898dad277dbf355945c2dee7f942d2a31ec1fc1455b6d4f552d07b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
